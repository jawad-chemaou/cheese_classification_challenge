# Cheese Classification challenge
This codebase allows you to jumpstart the INF473V challenge.
The goal of this channel is to create a cheese classifier without any real training data.
You will need to create your own training data from tools such as Stable Diffusion, SD-XL, etc...

## Instalation

Cloning the repo:
```
git clone git@github.com:nicolas-dufour/cheese_classification_challenge.git
cd cheese_classification_challenge
```
Install dependencies:
```
conda create -n cheese_challenge python=3.10
conda activate cheese_challenge
```
### Install requirements:
##### Install Pytorch:
If CUDA>=12.0:
```
pip install torch torchvision
```
If CUDA == 11.8
```
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 
```
Then install the rest of the requirements
```
pip install -r requirements.txt
```

Download the data from kaggle and copy them in the dataset folder
The data should be organized as follow: ```dataset/val```, ```dataset/test```. then the generated train sets will go to ```dataset/train/your_new_train_set```

## Using this codebase
This codebase is centered around 3 components: generating prompts for the generation of the synthetic training dataset, generating the training dataset and training your model.
Both rely on a config management library called hydra.


### Training

To train your model you can run 

```
python train.py
```

This will save a checkpoint in checkpoints with the name of the experiment you have. Careful, if you use the same exp name it will get overwritten

to change experiment name, you can do

```
python train.py experiment_name=new_experiment_name
```

### Generating prompts for the generation of the synthetic training dataset.

Prompts can be generated using the captions folder.

llama3_prompt_generator.py must be run in order to generate prompts using captions generated by BLIP-2. The captions are then saved in the example_dict.pkl dictionary in the main folder.
 
llava_prompt_generator.py can be run to generate prompts using captions generated by Llava (experimental) - an instance of Ollama must be downloaded beforehand.

### Creating Dreambooth LoRA models
In order to create specific LoRAs for each cheese, the following instructions must be followed: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md

All the code in the generators/diffusers folder was obtained by following these instructions and should be credited to its authors (mentioned in the precedent link).
To this cloned repo:
 - files other than dreambooth in the generators/diffusers/examples dir were deleted
 - the generators/diffusers/examples/dreambooth dir contains:
     - each LoRA model created with its checkpoints and relevant info. All of the models created are also available on https://huggingface.co/JawadC and can be used directly from the hub: thus, they were not uploaded locally.
     -  a val folder, containing the selected images from the validation dataset for training. This folder are available on Moodle.
     -  the train_dreambooth_lora_sdxl.py script, slightly modified to make it work. This script is also available on Moodle.
  
  In order to train a model for a specific cheese, you should run:
  ```
  accelerate launch config
  ```

  And then copy-paste the script in the script_dreambooth.py file in the main folder, into the terminal.
  
  As said previously, all LoRA models trained previously are available on https://huggingface.co/JawadC

### Generate images


You can generate datasets with the following command

```
python generate.py
```

If you want to create a new dataset generator method, write a method that inherits from `data.dataset_generators.base.DatasetGenerator` and create a new config file in `configs/generate/dataset_generator`.
You can then run

```
python generate.py dataset_generator=your_new_generator
```

### VRAM issues
If you have vram issues either use smaller diffusion models (SD 1.5) or try CPU offloading (much slower). For example for sdxl lightning you can do

```
python generate.py image_generator.use_cpu_offload=true
```

## Create submition
To create a submition file, you can run 
```
python create_submition.py experiment_name="name_of_the_exp_you_want_to_score" model=config_of_the_exp
```

Make sure to specify the name of the checkpoint you want to score and to have the right model config

## Bibliography
